{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docmhvr/Deep_Learning_with_Pytorch/blob/main/CIFAR10_CNN_Custom_Classifier/Assignment_CNN_CIFAR10_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW9Bg8A-JGaY",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">Assignment: Implement a CNN for Image Classification on CIFAR10 dataset</font>\n",
        "\n",
        "We have seen how to implement a CNN (LeNet5 and LeNet with the batch norm) in the last section. We used MNIST and Fashion MNIST dataset which are grayscale or single channel datasets. In this assignment, you will implement a CNN Model ( similar to LeNet ) for classifying objects in the `CIFAR10` dataset.\n",
        "\n",
        "The CIFAR10 dataset has the following properties\n",
        "1. It has `10` classes.  \n",
        "1. It has colored images, so it has `3-channels`.\n",
        "1. The image shape is `32 x 32`.\n",
        "\n",
        "Samples of CIFAR10- dataset ([source](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html?highlight=cifar)):\n",
        "\n",
        "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w3_cirar10.png\" width=700>\n",
        "\n",
        "\n",
        "# <font color='blue'>Marking Scheme</font>\n",
        "\n",
        "### <font style=\"color:green\">Maximum Points: 30\n",
        "\n",
        "<div>\n",
        "    <table>\n",
        "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Problem</h3></td> <td><h3>Points</h3></td> </tr>\n",
        "        <tr><td><h3>1</h3></td> <td><h3>Implement the CNN Model</h3></td> <td><h3>10</h3></td> </tr>\n",
        "        <tr><td><h3>2</h3></td> <td><h3>Find Mean and Std of Training Data</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>3</h3></td> <td><h3>Model Training & Accuracy</h3></td> <td><h3>15</h3></td> </tr>\n",
        "    </table>\n",
        "</div>\n",
        "\n",
        "\n",
        "# <font color='blue'>Problem Description</font>\n",
        "\n",
        "### <font color='blue'>1. Implement the CNN Model</font>\n",
        "Since the task is to classify objects in a dataset of color images, you need to implement a CNN with 10 output classes. **Also, your model must use `Conv2d`, `BatchNorm2d`, and `ReLU`.**\n",
        "\n",
        "**You need to define the model architecture in the function: `MyModel` ( Step 1 )**\n",
        "\n",
        "Hint: For color images you need to use an input shape that is different than the ones we have been using till now, so that it accepts 3 channel inputs.\n",
        "\n",
        "### <font color='blue'>2. Find Mean and Std of Training Data</font>\n",
        "\n",
        "It is a good practice to normalize the training data. To normalize the data, we need to compute mean and std. As the dataset has colored images, it has `3-channel` (RGB or BGR). We have to find mean and std per channel using training data.\n",
        "\n",
        "**You need to compute the mean and std for the dataset in the function: `get_mean_std_train_data` ( Step 3 )**\n",
        "\n",
        "### <font color='blue'>3. Model Training and Accuracy</font>\n",
        "\n",
        "Once you have defined the model, you can train it. To get better accuracy, you need to play around the training configuration **( Step 5 )** and even the model architecture. You can check the accuracy by running the training loop in `Step 11`.\n",
        "\n",
        "Here are a few hints on how you can improve the accuracy:\n",
        "- Train for longer duration\n",
        "- Try with different learning rate\n",
        "- Try to add more convolutional layers to the architecture\n",
        "- Try to add more nodes in the layers.\n",
        "\n",
        "You need to achieve **75% accuracy** ( See Step11 ) in order to get full marks for this part.\n",
        "\n",
        "**You do not need to implement anything for this, just changing the parameters as mentioned above and running the Notebook will give you the accuracy. ( Step 5 and Step 11 )**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJmbahxxJGah",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "required_training = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6H2Qu9FJGak",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mzyIelRJGao",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "import matplotlib.pyplot as plt  # one of the best graphics library for python"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUx0pTPAJGar",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from typing import Iterable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOLoiGVlJGav"
      },
      "source": [
        "# <font style=\"color:blue\">1. CNN Model Architecture [10 Points]</font>\n",
        "\n",
        "You have to write the model code here. You can take reference from LeNet code.\n",
        "\n",
        "If you do not get higher accuracy, here are a few hints:\n",
        "- Try to add more convolutional layers to the architecture\n",
        "- Try to add more nodes in the layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJygcZSaJGav"
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._body = nn.Sequential(\n",
        "            # Input 32x32 Output 30x30\n",
        "            nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # Input 15x15 Output 13x13\n",
        "            nn.Conv2d(in_channels=12, out_channels=36, kernel_size=3),\n",
        "            nn.BatchNorm2d(36),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # Input 6x6 Output 4x4\n",
        "            nn.Conv2d(in_channels=36, out_channels=108, kernel_size=3),\n",
        "            nn.BatchNorm2d(108),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "            # Output 2x2\n",
        "        )\n",
        "        self._head = nn.Sequential(\n",
        "            nn.Linear(in_features=432, out_features=324),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=324, out_features=210),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=210, out_features=96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=96, out_features=10) # Output Layer 10 classes for CIFAR-10\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._body(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self._head(x)\n",
        "        return x\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq46PTjsJGaz",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">2. Display the Network</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDEEFkVoJGa0",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "outputId": "41954a43-a64b-4594-8536-c1e9827616fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "my_model = MyModel()\n",
        "print(my_model)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyModel(\n",
            "  (_body): Sequential(\n",
            "    (0): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(12, 36, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(36, 108, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (9): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (_head): Sequential(\n",
            "    (0): Linear(in_features=432, out_features=324, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=324, out_features=210, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=210, out_features=96, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=96, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qR75AnbJGa3",
        "lines_to_next_cell": 2,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">3. Find Mean and STD of CIFAR10 Data [5 Points]</font>\n",
        "\n",
        "Function **`get_mean_std_train_data`** should `return` `mean` and `std` of training data. You can refer to the code used in the previous section for finding the mean and std of the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW7B7G5cJGa4",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "def get_mean_std_train_data(data_root):\n",
        "\n",
        "    train_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train_set = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_transform)\n",
        "\n",
        "    # return mean (numpy.ndarray) and std (numpy.ndarray)\n",
        "    # mean = train_set.mean()\n",
        "    # std = train_set.std()\n",
        "    # Using a data loader accumulate sums and squares of the images\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    for data in train_set:\n",
        "        image = data[0]  # Get the image tensor from the dataset\n",
        "        num_samples += 1\n",
        "        mean += image.mean(dim=(1, 2))\n",
        "        std += image.std(dim=(1, 2))\n",
        "\n",
        "    # Divide by the number of samples to get the mean and std\n",
        "    mean /= num_samples\n",
        "    std /= num_samples\n",
        "\n",
        "    return mean.numpy(), std.numpy()\n",
        "\n",
        "\n",
        "#     # Create a DataLoader to load data in batches\n",
        "#     train_loader = torch.utils.data.DataLoader(train_set, batch_size=512, shuffle=False, num_workers=2)\n",
        "\n",
        "#     # Initialize tensors to accumulate the sum of means and squared deviations\n",
        "#     mean = 0.0\n",
        "#     std = 0.0\n",
        "#     num_batches = len(train_loader)\n",
        "\n",
        "#     for images, _ in train_loader:\n",
        "#         # Flatten the batch to calculate mean and std across channels, height, and width\n",
        "#         batch_mean = torch.mean(images, dim=[0, 2, 3])\n",
        "#         batch_std = torch.std(images, dim=[0, 2, 3])\n",
        "#         mean += batch_mean\n",
        "#         std += batch_std\n",
        "\n",
        "#     # Average the sum of means and std over the number of batches\n",
        "#     mean /= num_batches\n",
        "#     std /= num_batches\n",
        "\n",
        "#     return mean.numpy(), std.numpy()"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsnJTJNXJGa7",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def get_data(batch_size, data_root, num_workers=1):\n",
        "\n",
        "\n",
        "    try:\n",
        "        mean, std = get_mean_std_train_data(data_root)\n",
        "        assert len(mean) == len(std) == 3\n",
        "    except:\n",
        "        mean = np.array([0.5, 0.5, 0.5])\n",
        "        std = np.array([0.5, 0.5, 0.5])\n",
        "\n",
        "\n",
        "    train_test_transforms = transforms.Compose([\n",
        "        # this re-scale image tensor values between 0-1. image_tensor /= 255\n",
        "        transforms.ToTensor(),\n",
        "        # subtract mean and divide by variance.\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    # train dataloader\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root=data_root, train=True, download=False, transform=train_test_transforms),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    # test dataloader\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root=data_root, train=False, download=False, transform=train_test_transforms),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03-ZuoXfJGa-",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">4. System Configuration</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4LsyamkJGa_",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "@dataclass\n",
        "class SystemConfiguration:\n",
        "    '''\n",
        "    Describes the common system setting needed for reproducible training\n",
        "    '''\n",
        "    seed: int = 42  # seed number to set the state of all random number generators\n",
        "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
        "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIIgMKe8JGbC",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">5. Training Configuration [15 Points]</font>\n",
        "All training parameters are defined here. So,\n",
        "This is where you can improve your accuracy, apart from improving the architecture.\n",
        "\n",
        "Here are a few hints on how you can improve the accuracy:\n",
        "- Train for longer duration\n",
        "- Try with different learning rate\n",
        "\n",
        "**You need to achieve 75% accuracy in order to get full marks for this part.**\n",
        "\n",
        "**You will see the effect of these changes when you run Step 11**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qESDffIJGbD",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "@dataclass\n",
        "class TrainingConfiguration:\n",
        "    '''\n",
        "    Describes configuration of the training process\n",
        "    '''\n",
        "    batch_size: int = 16  # amount of data to pass through the network at each forward-backward iteration\n",
        "    epochs_count: int = 10  # number of times the whole dataset will be passed through the network\n",
        "    learning_rate: float = 0.012  # determines the speed of network's weights update\n",
        "\n",
        "    log_interval: int = 100  # how many batches to wait between logging training status\n",
        "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
        "    data_root: str = \"./\"  # folder to save data\n",
        "    num_workers: int = 10  # number of concurrent processes using to prepare data\n",
        "    device: str = 'cuda'  # device to use for training.\n",
        "    # update changed parameters in blow coding block.\n",
        "    # Please do not change \"data_root\"\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    ### END SOLUTION\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAQE-iZMJGbG",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">6. System Setup</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdl_GzCqJGbG",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def setup_system(system_config: SystemConfiguration) -> None:\n",
        "    torch.manual_seed(system_config.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
        "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3OIPd3IJGbJ",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">7. Training</font>\n",
        "We are familiar with the training pipeline used in PyTorch. The following steps are performed in the code below:\n",
        "\n",
        "1. Send the data to the required device ( CPU/GPU )\n",
        "1. Make a forward pass using the forward method.\n",
        "1. Find the loss using the Cross_Entropy function.\n",
        "1. Find the gradients using the backward function.\n",
        "1. Update the weights using the optimizer.\n",
        "1. Find the accuracy of the model\n",
        "\n",
        "Repeat the above for the specified number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EEHzywxJGbK",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def train(\n",
        "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
        "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
        ") -> None:\n",
        "\n",
        "    # change model in training mood\n",
        "    model.train()\n",
        "\n",
        "    # to get batch loss\n",
        "    batch_loss = np.array([])\n",
        "\n",
        "    # to get batch accuracy\n",
        "    batch_acc = np.array([])\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # clone target\n",
        "        indx_target = target.clone()\n",
        "        # send data to device (its is medatory if GPU has to be used)\n",
        "        data = data.to(train_config.device)\n",
        "        # send target to device\n",
        "        target = target.to(train_config.device)\n",
        "\n",
        "        # reset parameters gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass to the model\n",
        "        output = model(data)\n",
        "\n",
        "        # cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # find gradients w.r.t training parameters\n",
        "        loss.backward()\n",
        "        # Update parameters using gardients\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss = np.append(batch_loss, [loss.item()])\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # correct prediction\n",
        "        correct = pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "        # accuracy\n",
        "        acc = float(correct) / float(len(data))\n",
        "\n",
        "        batch_acc = np.append(batch_acc, [acc])\n",
        "\n",
        "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
        "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
        "                )\n",
        "            )\n",
        "\n",
        "    epoch_loss = batch_loss.mean()\n",
        "    epoch_acc = batch_acc.mean()\n",
        "    return epoch_loss, epoch_acc"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQD5e4MvJGbN",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">8. Validation</font>\n",
        "\n",
        "After every few epochs **`validation`** will be called with the `trained model` and `test_loader` to get validation loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfYIabotJGbO",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def validate(\n",
        "    train_config: TrainingConfiguration,\n",
        "    model: nn.Module,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        ") -> float:\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    count_corect_predictions = 0\n",
        "    for data, target in test_loader:\n",
        "        indx_target = target.clone()\n",
        "        data = data.to(train_config.device)\n",
        "\n",
        "        target = target.to(train_config.device)\n",
        "\n",
        "        output = model(data)\n",
        "        # add loss for each mini batch\n",
        "        test_loss += F.cross_entropy(output, target).item()\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # add correct prediction count\n",
        "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "    # average over number of mini-batches\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    # average over number of dataset\n",
        "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
        "        )\n",
        "    )\n",
        "    return test_loss, accuracy/100.0"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kllVaLMYJGbV",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">9. Saving the Model</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX_5qui4JGbW",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def save_model(model, device, model_dir='models', model_file_name='cifar10_cnn_model.pt'):\n",
        "\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    model_path = os.path.join(model_dir, model_file_name)\n",
        "\n",
        "    # make sure you transfer the model to cpu.\n",
        "    if device == 'cuda':\n",
        "        model.to('cpu')\n",
        "\n",
        "    # save the state_dict\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        model.to('cuda')\n",
        "\n",
        "    return"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb-Z4XiHJGbZ",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">10. Main</font>\n",
        "\n",
        "In this section of code, we use the configuration parameters defined above and start the training. Here are the important actions being taken in the code below:\n",
        "\n",
        "1. Set up system parameters like CPU/GPU, number of threads etc\n",
        "1. Load the data using dataloaders\n",
        "1. Create an instance of the LeNet model\n",
        "1. Specify optimizer to use.\n",
        "1. Set up variables to track loss and accuracy and start training.\n",
        "1. If loss decreases, saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOKDxgpfJGbZ",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def main(system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
        "\n",
        "    # system configuration\n",
        "    setup_system(system_configuration)\n",
        "\n",
        "    # batch size\n",
        "    batch_size_to_set = training_configuration.batch_size\n",
        "    # num_workers\n",
        "    num_workers_to_set = training_configuration.num_workers\n",
        "    # epochs\n",
        "    epoch_num_to_set = training_configuration.epochs_count\n",
        "\n",
        "    # if GPU is available use training config,\n",
        "    # else lowers batch_size, num_workers and epochs count\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        num_workers_to_set = 2\n",
        "\n",
        "    # data loader\n",
        "    train_loader, test_loader = get_data(\n",
        "        batch_size=training_configuration.batch_size,\n",
        "        data_root=training_configuration.data_root,\n",
        "        num_workers=num_workers_to_set\n",
        "    )\n",
        "\n",
        "    # Update training configuration\n",
        "    training_configuration = TrainingConfiguration(\n",
        "        device=device,\n",
        "        num_workers=num_workers_to_set\n",
        "    )\n",
        "\n",
        "    # initiate model\n",
        "    model = MyModel()\n",
        "\n",
        "    print(\"Training on {}\".format(training_configuration.device))\n",
        "\n",
        "    # send model to device (GPU/CPU)\n",
        "    model.to(training_configuration.device)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=training_configuration.learning_rate\n",
        "    )\n",
        "\n",
        "    best_loss = torch.tensor(np.inf)\n",
        "    best_accuracy = torch.tensor(0)\n",
        "\n",
        "    # epoch train/test loss\n",
        "    epoch_train_loss = np.array([])\n",
        "    epoch_test_loss = np.array([])\n",
        "\n",
        "    # epch train/test accuracy\n",
        "    epoch_train_acc = np.array([])\n",
        "    epoch_test_acc = np.array([])\n",
        "\n",
        "    # trainig time measurement\n",
        "    t_begin = time.time()\n",
        "    for epoch in range(training_configuration.epochs_count):\n",
        "\n",
        "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
        "\n",
        "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
        "\n",
        "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
        "\n",
        "        elapsed_time = time.time() - t_begin\n",
        "        speed_epoch = elapsed_time / (epoch + 1)\n",
        "        speed_batch = speed_epoch / len(train_loader)\n",
        "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
        "\n",
        "        print(\n",
        "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
        "                elapsed_time, speed_epoch, speed_batch, eta\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if epoch % training_configuration.test_interval == 0:\n",
        "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
        "\n",
        "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
        "\n",
        "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
        "\n",
        "            if current_loss < best_loss:\n",
        "                best_loss = current_loss\n",
        "\n",
        "            if current_accuracy > best_accuracy:\n",
        "                best_accuracy = current_accuracy\n",
        "                print('Accuracy improved, saving the model.\\n')\n",
        "                save_model(model, device)\n",
        "\n",
        "\n",
        "    print(\"Total time: {:.2f}, Best Loss: {:.3f}, Best Accuracy: {:.3f}\".format(time.time() - t_begin, best_loss,\n",
        "                                                                                best_accuracy))\n",
        "\n",
        "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zll4UE--JGbd"
      },
      "source": [
        "# <font style=\"color:blue\">Step 11: Start Training</font>\n",
        "This is where you start the training. You may see that the training does not converge or does not give a good accuracy. You need to change\n",
        "- In Step 1: the network architecture and add a few more layers or more nodes to the already existing layers\n",
        "- In Step 5: training parameters such as learning rate or batch_size or epochs so that the network converges or run the network for longer so that it gets more time to fit the data\n",
        "\n",
        "**You need to make sure that the accuracy at the end is at least 75%.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X13F4qQCJGbe",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "scrolled": true,
        "outputId": "6dc66d54-5388-4b30-9330-06983ea4b6b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if required_training:\n",
        "    model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Training on cuda\n",
            "Train Epoch: 0 [1600/50000] Loss: 2.296417 Acc: 0.1250\n",
            "Train Epoch: 0 [3200/50000] Loss: 2.270240 Acc: 0.1875\n",
            "Train Epoch: 0 [4800/50000] Loss: 2.153744 Acc: 0.2500\n",
            "Train Epoch: 0 [6400/50000] Loss: 2.018113 Acc: 0.5000\n",
            "Train Epoch: 0 [8000/50000] Loss: 1.884936 Acc: 0.2500\n",
            "Train Epoch: 0 [9600/50000] Loss: 1.892598 Acc: 0.0625\n",
            "Train Epoch: 0 [11200/50000] Loss: 1.813995 Acc: 0.1875\n",
            "Train Epoch: 0 [12800/50000] Loss: 1.730392 Acc: 0.3125\n",
            "Train Epoch: 0 [14400/50000] Loss: 1.765916 Acc: 0.2500\n",
            "Train Epoch: 0 [16000/50000] Loss: 1.847194 Acc: 0.2500\n",
            "Train Epoch: 0 [17600/50000] Loss: 1.829209 Acc: 0.5000\n",
            "Train Epoch: 0 [19200/50000] Loss: 1.430112 Acc: 0.2500\n",
            "Train Epoch: 0 [20800/50000] Loss: 1.569937 Acc: 0.2500\n",
            "Train Epoch: 0 [22400/50000] Loss: 1.464720 Acc: 0.3750\n",
            "Train Epoch: 0 [24000/50000] Loss: 1.570610 Acc: 0.2500\n",
            "Train Epoch: 0 [25600/50000] Loss: 1.802494 Acc: 0.1250\n",
            "Train Epoch: 0 [27200/50000] Loss: 1.611423 Acc: 0.3125\n",
            "Train Epoch: 0 [28800/50000] Loss: 1.968123 Acc: 0.2500\n",
            "Train Epoch: 0 [30400/50000] Loss: 1.616906 Acc: 0.3750\n",
            "Train Epoch: 0 [32000/50000] Loss: 1.292385 Acc: 0.4375\n",
            "Train Epoch: 0 [33600/50000] Loss: 1.741654 Acc: 0.3750\n",
            "Train Epoch: 0 [35200/50000] Loss: 1.261370 Acc: 0.6250\n",
            "Train Epoch: 0 [36800/50000] Loss: 1.251851 Acc: 0.6250\n",
            "Train Epoch: 0 [38400/50000] Loss: 0.944885 Acc: 0.5625\n",
            "Train Epoch: 0 [40000/50000] Loss: 1.691066 Acc: 0.4375\n",
            "Train Epoch: 0 [41600/50000] Loss: 1.440148 Acc: 0.4375\n",
            "Train Epoch: 0 [43200/50000] Loss: 1.331190 Acc: 0.5625\n",
            "Train Epoch: 0 [44800/50000] Loss: 0.985532 Acc: 0.5625\n",
            "Train Epoch: 0 [46400/50000] Loss: 1.282265 Acc: 0.5625\n",
            "Train Epoch: 0 [48000/50000] Loss: 1.335810 Acc: 0.5000\n",
            "Train Epoch: 0 [49600/50000] Loss: 1.585061 Acc: 0.2500\n",
            "Elapsed 31.34s, 31.34 s/epoch, 0.01 s/batch, ets 282.02s\n",
            "\n",
            "Test set: Average loss: 1.3044, Accuracy: 5233/10000 (52%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 1 [1600/50000] Loss: 1.700985 Acc: 0.3750\n",
            "Train Epoch: 1 [3200/50000] Loss: 1.060351 Acc: 0.5625\n",
            "Train Epoch: 1 [4800/50000] Loss: 1.447363 Acc: 0.3125\n",
            "Train Epoch: 1 [6400/50000] Loss: 1.213562 Acc: 0.5625\n",
            "Train Epoch: 1 [8000/50000] Loss: 1.065307 Acc: 0.5625\n",
            "Train Epoch: 1 [9600/50000] Loss: 1.217981 Acc: 0.4375\n",
            "Train Epoch: 1 [11200/50000] Loss: 0.993636 Acc: 0.6250\n",
            "Train Epoch: 1 [12800/50000] Loss: 1.283545 Acc: 0.5625\n",
            "Train Epoch: 1 [14400/50000] Loss: 0.871356 Acc: 0.5625\n",
            "Train Epoch: 1 [16000/50000] Loss: 1.095210 Acc: 0.6250\n",
            "Train Epoch: 1 [17600/50000] Loss: 1.367596 Acc: 0.5000\n",
            "Train Epoch: 1 [19200/50000] Loss: 1.521078 Acc: 0.4375\n",
            "Train Epoch: 1 [20800/50000] Loss: 0.946860 Acc: 0.6250\n",
            "Train Epoch: 1 [22400/50000] Loss: 0.932829 Acc: 0.5625\n",
            "Train Epoch: 1 [24000/50000] Loss: 1.616564 Acc: 0.5000\n",
            "Train Epoch: 1 [25600/50000] Loss: 0.705845 Acc: 0.7500\n",
            "Train Epoch: 1 [27200/50000] Loss: 0.807008 Acc: 0.6875\n",
            "Train Epoch: 1 [28800/50000] Loss: 1.402260 Acc: 0.3750\n",
            "Train Epoch: 1 [30400/50000] Loss: 1.099487 Acc: 0.5000\n",
            "Train Epoch: 1 [32000/50000] Loss: 1.269078 Acc: 0.5625\n",
            "Train Epoch: 1 [33600/50000] Loss: 1.253026 Acc: 0.4375\n",
            "Train Epoch: 1 [35200/50000] Loss: 1.000611 Acc: 0.6250\n",
            "Train Epoch: 1 [36800/50000] Loss: 1.089098 Acc: 0.6875\n",
            "Train Epoch: 1 [38400/50000] Loss: 1.221081 Acc: 0.4375\n",
            "Train Epoch: 1 [40000/50000] Loss: 1.044939 Acc: 0.5000\n",
            "Train Epoch: 1 [41600/50000] Loss: 1.169214 Acc: 0.5000\n",
            "Train Epoch: 1 [43200/50000] Loss: 1.071139 Acc: 0.5625\n",
            "Train Epoch: 1 [44800/50000] Loss: 0.904891 Acc: 0.6875\n",
            "Train Epoch: 1 [46400/50000] Loss: 1.209637 Acc: 0.5000\n",
            "Train Epoch: 1 [48000/50000] Loss: 1.330177 Acc: 0.6250\n",
            "Train Epoch: 1 [49600/50000] Loss: 1.102648 Acc: 0.5625\n",
            "Elapsed 66.36s, 33.18 s/epoch, 0.01 s/batch, ets 265.43s\n",
            "\n",
            "Test set: Average loss: 1.1113, Accuracy: 6003/10000 (60%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 2 [1600/50000] Loss: 1.514313 Acc: 0.4375\n",
            "Train Epoch: 2 [3200/50000] Loss: 1.253654 Acc: 0.6250\n",
            "Train Epoch: 2 [4800/50000] Loss: 1.291239 Acc: 0.5625\n",
            "Train Epoch: 2 [6400/50000] Loss: 0.823125 Acc: 0.5625\n",
            "Train Epoch: 2 [8000/50000] Loss: 1.253556 Acc: 0.5625\n",
            "Train Epoch: 2 [9600/50000] Loss: 0.821776 Acc: 0.6875\n",
            "Train Epoch: 2 [11200/50000] Loss: 1.176676 Acc: 0.6250\n",
            "Train Epoch: 2 [12800/50000] Loss: 1.339211 Acc: 0.5625\n",
            "Train Epoch: 2 [14400/50000] Loss: 1.261556 Acc: 0.5000\n",
            "Train Epoch: 2 [16000/50000] Loss: 0.870570 Acc: 0.6875\n",
            "Train Epoch: 2 [17600/50000] Loss: 0.717872 Acc: 0.6875\n",
            "Train Epoch: 2 [19200/50000] Loss: 1.056662 Acc: 0.5000\n",
            "Train Epoch: 2 [20800/50000] Loss: 1.457088 Acc: 0.5625\n",
            "Train Epoch: 2 [22400/50000] Loss: 0.964217 Acc: 0.5625\n",
            "Train Epoch: 2 [24000/50000] Loss: 0.875038 Acc: 0.6875\n",
            "Train Epoch: 2 [25600/50000] Loss: 0.787461 Acc: 0.6875\n",
            "Train Epoch: 2 [27200/50000] Loss: 0.926233 Acc: 0.7500\n",
            "Train Epoch: 2 [28800/50000] Loss: 1.201571 Acc: 0.5000\n",
            "Train Epoch: 2 [30400/50000] Loss: 1.220390 Acc: 0.5625\n",
            "Train Epoch: 2 [32000/50000] Loss: 1.120545 Acc: 0.5625\n",
            "Train Epoch: 2 [33600/50000] Loss: 1.039998 Acc: 0.6250\n",
            "Train Epoch: 2 [35200/50000] Loss: 0.884803 Acc: 0.7500\n",
            "Train Epoch: 2 [36800/50000] Loss: 0.887084 Acc: 0.6875\n",
            "Train Epoch: 2 [38400/50000] Loss: 1.137458 Acc: 0.6875\n",
            "Train Epoch: 2 [40000/50000] Loss: 1.254709 Acc: 0.5000\n",
            "Train Epoch: 2 [41600/50000] Loss: 1.232163 Acc: 0.5000\n",
            "Train Epoch: 2 [43200/50000] Loss: 0.643790 Acc: 0.8125\n",
            "Train Epoch: 2 [44800/50000] Loss: 0.796699 Acc: 0.7500\n",
            "Train Epoch: 2 [46400/50000] Loss: 0.923026 Acc: 0.6250\n",
            "Train Epoch: 2 [48000/50000] Loss: 0.883498 Acc: 0.6250\n",
            "Train Epoch: 2 [49600/50000] Loss: 1.307149 Acc: 0.6250\n",
            "Elapsed 103.28s, 34.43 s/epoch, 0.01 s/batch, ets 241.00s\n",
            "\n",
            "Test set: Average loss: 0.9935, Accuracy: 6484/10000 (65%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 3 [1600/50000] Loss: 0.764988 Acc: 0.8750\n",
            "Train Epoch: 3 [3200/50000] Loss: 0.668674 Acc: 0.7500\n",
            "Train Epoch: 3 [4800/50000] Loss: 0.485868 Acc: 0.8750\n",
            "Train Epoch: 3 [6400/50000] Loss: 0.817091 Acc: 0.6250\n",
            "Train Epoch: 3 [8000/50000] Loss: 0.746160 Acc: 0.8125\n",
            "Train Epoch: 3 [9600/50000] Loss: 1.065688 Acc: 0.7500\n",
            "Train Epoch: 3 [11200/50000] Loss: 1.363572 Acc: 0.5625\n",
            "Train Epoch: 3 [12800/50000] Loss: 1.171921 Acc: 0.6250\n",
            "Train Epoch: 3 [14400/50000] Loss: 0.728163 Acc: 0.6250\n",
            "Train Epoch: 3 [16000/50000] Loss: 1.243842 Acc: 0.4375\n",
            "Train Epoch: 3 [17600/50000] Loss: 1.092516 Acc: 0.5625\n",
            "Train Epoch: 3 [19200/50000] Loss: 0.842526 Acc: 0.6875\n",
            "Train Epoch: 3 [20800/50000] Loss: 1.276608 Acc: 0.5000\n",
            "Train Epoch: 3 [22400/50000] Loss: 0.703570 Acc: 0.6875\n",
            "Train Epoch: 3 [24000/50000] Loss: 0.576678 Acc: 0.7500\n",
            "Train Epoch: 3 [25600/50000] Loss: 0.828068 Acc: 0.6250\n",
            "Train Epoch: 3 [27200/50000] Loss: 0.534564 Acc: 0.8125\n",
            "Train Epoch: 3 [28800/50000] Loss: 0.834661 Acc: 0.5625\n",
            "Train Epoch: 3 [30400/50000] Loss: 1.413934 Acc: 0.5000\n",
            "Train Epoch: 3 [32000/50000] Loss: 0.853585 Acc: 0.8125\n",
            "Train Epoch: 3 [33600/50000] Loss: 0.852063 Acc: 0.6875\n",
            "Train Epoch: 3 [35200/50000] Loss: 0.684589 Acc: 0.7500\n",
            "Train Epoch: 3 [36800/50000] Loss: 0.551323 Acc: 0.8125\n",
            "Train Epoch: 3 [38400/50000] Loss: 1.033809 Acc: 0.6250\n",
            "Train Epoch: 3 [40000/50000] Loss: 0.642944 Acc: 0.8125\n",
            "Train Epoch: 3 [41600/50000] Loss: 1.132721 Acc: 0.6250\n",
            "Train Epoch: 3 [43200/50000] Loss: 1.006978 Acc: 0.5625\n",
            "Train Epoch: 3 [44800/50000] Loss: 0.693323 Acc: 0.7500\n",
            "Train Epoch: 3 [46400/50000] Loss: 1.126138 Acc: 0.5000\n",
            "Train Epoch: 3 [48000/50000] Loss: 0.550143 Acc: 0.9375\n",
            "Train Epoch: 3 [49600/50000] Loss: 0.928114 Acc: 0.6875\n",
            "Elapsed 140.01s, 35.00 s/epoch, 0.01 s/batch, ets 210.01s\n",
            "\n",
            "Test set: Average loss: 0.8825, Accuracy: 6885/10000 (69%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 4 [1600/50000] Loss: 0.591504 Acc: 0.7500\n",
            "Train Epoch: 4 [3200/50000] Loss: 0.795633 Acc: 0.6875\n",
            "Train Epoch: 4 [4800/50000] Loss: 0.784449 Acc: 0.5625\n",
            "Train Epoch: 4 [6400/50000] Loss: 0.534355 Acc: 0.9375\n",
            "Train Epoch: 4 [8000/50000] Loss: 0.657539 Acc: 0.8125\n",
            "Train Epoch: 4 [9600/50000] Loss: 0.588623 Acc: 0.8125\n",
            "Train Epoch: 4 [11200/50000] Loss: 0.691651 Acc: 0.6875\n",
            "Train Epoch: 4 [12800/50000] Loss: 0.948538 Acc: 0.6875\n",
            "Train Epoch: 4 [14400/50000] Loss: 0.988903 Acc: 0.6875\n",
            "Train Epoch: 4 [16000/50000] Loss: 0.773172 Acc: 0.6875\n",
            "Train Epoch: 4 [17600/50000] Loss: 0.795593 Acc: 0.7500\n",
            "Train Epoch: 4 [19200/50000] Loss: 1.626558 Acc: 0.3750\n",
            "Train Epoch: 4 [20800/50000] Loss: 0.679691 Acc: 0.8750\n",
            "Train Epoch: 4 [22400/50000] Loss: 1.035196 Acc: 0.6250\n",
            "Train Epoch: 4 [24000/50000] Loss: 0.833387 Acc: 0.6250\n",
            "Train Epoch: 4 [25600/50000] Loss: 0.369015 Acc: 0.8125\n",
            "Train Epoch: 4 [27200/50000] Loss: 1.079701 Acc: 0.7500\n",
            "Train Epoch: 4 [28800/50000] Loss: 0.936440 Acc: 0.6250\n",
            "Train Epoch: 4 [30400/50000] Loss: 0.908651 Acc: 0.6250\n",
            "Train Epoch: 4 [32000/50000] Loss: 0.851152 Acc: 0.6250\n",
            "Train Epoch: 4 [33600/50000] Loss: 0.889168 Acc: 0.5000\n",
            "Train Epoch: 4 [35200/50000] Loss: 0.483739 Acc: 0.8750\n",
            "Train Epoch: 4 [36800/50000] Loss: 0.803986 Acc: 0.6875\n",
            "Train Epoch: 4 [38400/50000] Loss: 1.267619 Acc: 0.5625\n",
            "Train Epoch: 4 [40000/50000] Loss: 0.605936 Acc: 0.8125\n",
            "Train Epoch: 4 [41600/50000] Loss: 0.619601 Acc: 0.8125\n",
            "Train Epoch: 4 [43200/50000] Loss: 0.338071 Acc: 0.9375\n",
            "Train Epoch: 4 [44800/50000] Loss: 1.027521 Acc: 0.6250\n",
            "Train Epoch: 4 [46400/50000] Loss: 0.789733 Acc: 0.6250\n",
            "Train Epoch: 4 [48000/50000] Loss: 0.902558 Acc: 0.7500\n",
            "Train Epoch: 4 [49600/50000] Loss: 1.285345 Acc: 0.6250\n",
            "Elapsed 176.76s, 35.35 s/epoch, 0.01 s/batch, ets 176.76s\n",
            "\n",
            "Test set: Average loss: 0.8560, Accuracy: 6980/10000 (70%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 5 [1600/50000] Loss: 0.852220 Acc: 0.6875\n",
            "Train Epoch: 5 [3200/50000] Loss: 0.815647 Acc: 0.8750\n",
            "Train Epoch: 5 [4800/50000] Loss: 1.072251 Acc: 0.5000\n",
            "Train Epoch: 5 [6400/50000] Loss: 0.735670 Acc: 0.6875\n",
            "Train Epoch: 5 [8000/50000] Loss: 0.773217 Acc: 0.7500\n",
            "Train Epoch: 5 [9600/50000] Loss: 0.833539 Acc: 0.8125\n",
            "Train Epoch: 5 [11200/50000] Loss: 0.289282 Acc: 0.9375\n",
            "Train Epoch: 5 [12800/50000] Loss: 0.539604 Acc: 0.7500\n",
            "Train Epoch: 5 [14400/50000] Loss: 0.370582 Acc: 0.8125\n",
            "Train Epoch: 5 [16000/50000] Loss: 0.800675 Acc: 0.7500\n",
            "Train Epoch: 5 [17600/50000] Loss: 1.074123 Acc: 0.6250\n",
            "Train Epoch: 5 [19200/50000] Loss: 0.524467 Acc: 0.8125\n",
            "Train Epoch: 5 [20800/50000] Loss: 1.084435 Acc: 0.6875\n",
            "Train Epoch: 5 [22400/50000] Loss: 0.452871 Acc: 0.8125\n",
            "Train Epoch: 5 [24000/50000] Loss: 0.818798 Acc: 0.6875\n",
            "Train Epoch: 5 [25600/50000] Loss: 0.276672 Acc: 0.9375\n",
            "Train Epoch: 5 [27200/50000] Loss: 0.657541 Acc: 0.6875\n",
            "Train Epoch: 5 [28800/50000] Loss: 0.494524 Acc: 0.8750\n",
            "Train Epoch: 5 [30400/50000] Loss: 1.049560 Acc: 0.6875\n",
            "Train Epoch: 5 [32000/50000] Loss: 0.698273 Acc: 0.6875\n",
            "Train Epoch: 5 [33600/50000] Loss: 0.940982 Acc: 0.7500\n",
            "Train Epoch: 5 [35200/50000] Loss: 0.356898 Acc: 0.9375\n",
            "Train Epoch: 5 [36800/50000] Loss: 0.578744 Acc: 0.7500\n",
            "Train Epoch: 5 [38400/50000] Loss: 0.372204 Acc: 0.9375\n",
            "Train Epoch: 5 [40000/50000] Loss: 0.547004 Acc: 0.8125\n",
            "Train Epoch: 5 [41600/50000] Loss: 1.103946 Acc: 0.6875\n",
            "Train Epoch: 5 [43200/50000] Loss: 0.932690 Acc: 0.6250\n",
            "Train Epoch: 5 [44800/50000] Loss: 0.909622 Acc: 0.6250\n",
            "Train Epoch: 5 [46400/50000] Loss: 0.509143 Acc: 0.8750\n",
            "Train Epoch: 5 [48000/50000] Loss: 0.741375 Acc: 0.6875\n",
            "Train Epoch: 5 [49600/50000] Loss: 0.836704 Acc: 0.7500\n",
            "Elapsed 211.67s, 35.28 s/epoch, 0.01 s/batch, ets 141.11s\n",
            "\n",
            "Test set: Average loss: 0.8446, Accuracy: 7098/10000 (71%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 6 [1600/50000] Loss: 0.833401 Acc: 0.6875\n",
            "Train Epoch: 6 [3200/50000] Loss: 0.696977 Acc: 0.7500\n",
            "Train Epoch: 6 [4800/50000] Loss: 0.435694 Acc: 0.8125\n",
            "Train Epoch: 6 [6400/50000] Loss: 0.905392 Acc: 0.6875\n",
            "Train Epoch: 6 [8000/50000] Loss: 0.555774 Acc: 0.7500\n",
            "Train Epoch: 6 [9600/50000] Loss: 1.479801 Acc: 0.4375\n",
            "Train Epoch: 6 [11200/50000] Loss: 1.043362 Acc: 0.4375\n",
            "Train Epoch: 6 [12800/50000] Loss: 0.319114 Acc: 0.9375\n",
            "Train Epoch: 6 [14400/50000] Loss: 0.838549 Acc: 0.6875\n",
            "Train Epoch: 6 [16000/50000] Loss: 0.723631 Acc: 0.7500\n",
            "Train Epoch: 6 [17600/50000] Loss: 0.864227 Acc: 0.7500\n",
            "Train Epoch: 6 [19200/50000] Loss: 0.506702 Acc: 0.8125\n",
            "Train Epoch: 6 [20800/50000] Loss: 0.800246 Acc: 0.7500\n",
            "Train Epoch: 6 [22400/50000] Loss: 0.604862 Acc: 0.8750\n",
            "Train Epoch: 6 [24000/50000] Loss: 0.808211 Acc: 0.6250\n",
            "Train Epoch: 6 [25600/50000] Loss: 0.378009 Acc: 0.8750\n",
            "Train Epoch: 6 [27200/50000] Loss: 0.983323 Acc: 0.5625\n",
            "Train Epoch: 6 [28800/50000] Loss: 0.359186 Acc: 1.0000\n",
            "Train Epoch: 6 [30400/50000] Loss: 0.548447 Acc: 0.8125\n",
            "Train Epoch: 6 [32000/50000] Loss: 0.639314 Acc: 0.7500\n",
            "Train Epoch: 6 [33600/50000] Loss: 0.449403 Acc: 0.8750\n",
            "Train Epoch: 6 [35200/50000] Loss: 0.856693 Acc: 0.5625\n",
            "Train Epoch: 6 [36800/50000] Loss: 0.888927 Acc: 0.6875\n",
            "Train Epoch: 6 [38400/50000] Loss: 0.475672 Acc: 0.8125\n",
            "Train Epoch: 6 [40000/50000] Loss: 1.148410 Acc: 0.7500\n",
            "Train Epoch: 6 [41600/50000] Loss: 1.012916 Acc: 0.6250\n",
            "Train Epoch: 6 [43200/50000] Loss: 0.241983 Acc: 0.9375\n",
            "Train Epoch: 6 [44800/50000] Loss: 0.522740 Acc: 0.8125\n",
            "Train Epoch: 6 [46400/50000] Loss: 0.274172 Acc: 1.0000\n",
            "Train Epoch: 6 [48000/50000] Loss: 0.541470 Acc: 0.8750\n",
            "Train Epoch: 6 [49600/50000] Loss: 0.447290 Acc: 0.8125\n",
            "Elapsed 249.49s, 35.64 s/epoch, 0.01 s/batch, ets 106.92s\n",
            "\n",
            "Test set: Average loss: 0.8336, Accuracy: 7063/10000 (71%)\n",
            "\n",
            "Train Epoch: 7 [1600/50000] Loss: 1.127993 Acc: 0.6875\n",
            "Train Epoch: 7 [3200/50000] Loss: 0.496219 Acc: 0.8750\n",
            "Train Epoch: 7 [4800/50000] Loss: 0.451909 Acc: 0.8750\n",
            "Train Epoch: 7 [6400/50000] Loss: 0.435664 Acc: 0.9375\n",
            "Train Epoch: 7 [8000/50000] Loss: 0.479211 Acc: 0.8750\n",
            "Train Epoch: 7 [9600/50000] Loss: 0.784307 Acc: 0.8750\n",
            "Train Epoch: 7 [11200/50000] Loss: 0.826979 Acc: 0.8125\n",
            "Train Epoch: 7 [12800/50000] Loss: 0.851771 Acc: 0.6250\n",
            "Train Epoch: 7 [14400/50000] Loss: 0.435683 Acc: 0.8750\n",
            "Train Epoch: 7 [16000/50000] Loss: 0.709634 Acc: 0.6250\n",
            "Train Epoch: 7 [17600/50000] Loss: 0.547060 Acc: 0.8750\n",
            "Train Epoch: 7 [19200/50000] Loss: 0.670599 Acc: 0.8125\n",
            "Train Epoch: 7 [20800/50000] Loss: 0.935467 Acc: 0.7500\n",
            "Train Epoch: 7 [22400/50000] Loss: 1.099252 Acc: 0.7500\n",
            "Train Epoch: 7 [24000/50000] Loss: 0.668924 Acc: 0.7500\n",
            "Train Epoch: 7 [25600/50000] Loss: 0.541672 Acc: 0.8125\n",
            "Train Epoch: 7 [27200/50000] Loss: 0.753651 Acc: 0.7500\n",
            "Train Epoch: 7 [28800/50000] Loss: 0.846441 Acc: 0.7500\n",
            "Train Epoch: 7 [30400/50000] Loss: 0.687840 Acc: 0.7500\n",
            "Train Epoch: 7 [32000/50000] Loss: 0.984923 Acc: 0.7500\n",
            "Train Epoch: 7 [33600/50000] Loss: 0.492177 Acc: 0.8125\n",
            "Train Epoch: 7 [35200/50000] Loss: 0.919710 Acc: 0.6875\n",
            "Train Epoch: 7 [36800/50000] Loss: 0.291006 Acc: 0.9375\n",
            "Train Epoch: 7 [38400/50000] Loss: 0.546834 Acc: 0.7500\n",
            "Train Epoch: 7 [40000/50000] Loss: 1.340125 Acc: 0.6875\n",
            "Train Epoch: 7 [41600/50000] Loss: 0.622991 Acc: 0.8125\n",
            "Train Epoch: 7 [43200/50000] Loss: 0.288286 Acc: 0.9375\n",
            "Train Epoch: 7 [44800/50000] Loss: 1.054228 Acc: 0.6250\n",
            "Train Epoch: 7 [46400/50000] Loss: 0.295377 Acc: 0.9375\n",
            "Train Epoch: 7 [48000/50000] Loss: 0.849621 Acc: 0.8125\n",
            "Train Epoch: 7 [49600/50000] Loss: 1.209321 Acc: 0.5625\n",
            "Elapsed 284.67s, 35.58 s/epoch, 0.01 s/batch, ets 71.17s\n",
            "\n",
            "Test set: Average loss: 0.7840, Accuracy: 7333/10000 (73%)\n",
            "\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Train Epoch: 8 [1600/50000] Loss: 0.584776 Acc: 0.7500\n",
            "Train Epoch: 8 [3200/50000] Loss: 0.663796 Acc: 0.8125\n",
            "Train Epoch: 8 [4800/50000] Loss: 1.254019 Acc: 0.5000\n",
            "Train Epoch: 8 [6400/50000] Loss: 0.900769 Acc: 0.7500\n",
            "Train Epoch: 8 [8000/50000] Loss: 0.808554 Acc: 0.6875\n",
            "Train Epoch: 8 [9600/50000] Loss: 0.776852 Acc: 0.6875\n",
            "Train Epoch: 8 [11200/50000] Loss: 1.601217 Acc: 0.5000\n",
            "Train Epoch: 8 [12800/50000] Loss: 0.689861 Acc: 0.6250\n",
            "Train Epoch: 8 [14400/50000] Loss: 0.832683 Acc: 0.6875\n",
            "Train Epoch: 8 [16000/50000] Loss: 0.472575 Acc: 0.9375\n",
            "Train Epoch: 8 [17600/50000] Loss: 0.720304 Acc: 0.6875\n",
            "Train Epoch: 8 [19200/50000] Loss: 0.786103 Acc: 0.7500\n",
            "Train Epoch: 8 [20800/50000] Loss: 0.598892 Acc: 0.7500\n",
            "Train Epoch: 8 [22400/50000] Loss: 0.855277 Acc: 0.6875\n",
            "Train Epoch: 8 [24000/50000] Loss: 0.605094 Acc: 0.9375\n",
            "Train Epoch: 8 [25600/50000] Loss: 0.481731 Acc: 0.8125\n",
            "Train Epoch: 8 [27200/50000] Loss: 0.838432 Acc: 0.7500\n",
            "Train Epoch: 8 [28800/50000] Loss: 0.618798 Acc: 0.7500\n",
            "Train Epoch: 8 [30400/50000] Loss: 0.357556 Acc: 0.8750\n",
            "Train Epoch: 8 [32000/50000] Loss: 0.406491 Acc: 0.9375\n",
            "Train Epoch: 8 [33600/50000] Loss: 0.988729 Acc: 0.6250\n",
            "Train Epoch: 8 [35200/50000] Loss: 0.306860 Acc: 0.8750\n",
            "Train Epoch: 8 [36800/50000] Loss: 0.699172 Acc: 0.7500\n",
            "Train Epoch: 8 [38400/50000] Loss: 0.599718 Acc: 0.7500\n",
            "Train Epoch: 8 [40000/50000] Loss: 0.351831 Acc: 0.8750\n",
            "Train Epoch: 8 [41600/50000] Loss: 0.545089 Acc: 0.7500\n",
            "Train Epoch: 8 [43200/50000] Loss: 0.379323 Acc: 0.8750\n",
            "Train Epoch: 8 [44800/50000] Loss: 0.177982 Acc: 1.0000\n",
            "Train Epoch: 8 [46400/50000] Loss: 0.424250 Acc: 0.8750\n",
            "Train Epoch: 8 [48000/50000] Loss: 0.593193 Acc: 0.6875\n",
            "Train Epoch: 8 [49600/50000] Loss: 0.990583 Acc: 0.6250\n",
            "Elapsed 323.01s, 35.89 s/epoch, 0.01 s/batch, ets 35.89s\n",
            "\n",
            "Test set: Average loss: 0.8386, Accuracy: 7188/10000 (72%)\n",
            "\n",
            "Train Epoch: 9 [1600/50000] Loss: 0.679549 Acc: 0.8750\n",
            "Train Epoch: 9 [3200/50000] Loss: 0.319113 Acc: 0.9375\n",
            "Train Epoch: 9 [4800/50000] Loss: 0.209181 Acc: 0.9375\n",
            "Train Epoch: 9 [6400/50000] Loss: 0.335910 Acc: 0.8750\n",
            "Train Epoch: 9 [8000/50000] Loss: 0.408549 Acc: 0.8750\n",
            "Train Epoch: 9 [9600/50000] Loss: 0.506635 Acc: 0.6875\n",
            "Train Epoch: 9 [11200/50000] Loss: 0.761913 Acc: 0.8750\n",
            "Train Epoch: 9 [12800/50000] Loss: 0.700838 Acc: 0.7500\n",
            "Train Epoch: 9 [14400/50000] Loss: 0.636195 Acc: 0.7500\n",
            "Train Epoch: 9 [16000/50000] Loss: 0.456808 Acc: 0.7500\n",
            "Train Epoch: 9 [17600/50000] Loss: 0.220444 Acc: 0.9375\n",
            "Train Epoch: 9 [19200/50000] Loss: 0.376220 Acc: 0.8750\n",
            "Train Epoch: 9 [20800/50000] Loss: 0.622608 Acc: 0.6875\n",
            "Train Epoch: 9 [22400/50000] Loss: 0.595821 Acc: 0.8125\n",
            "Train Epoch: 9 [24000/50000] Loss: 0.640504 Acc: 0.6250\n",
            "Train Epoch: 9 [25600/50000] Loss: 0.340517 Acc: 0.9375\n",
            "Train Epoch: 9 [27200/50000] Loss: 0.936728 Acc: 0.7500\n",
            "Train Epoch: 9 [28800/50000] Loss: 0.392226 Acc: 0.8750\n",
            "Train Epoch: 9 [30400/50000] Loss: 0.412166 Acc: 0.8125\n",
            "Train Epoch: 9 [32000/50000] Loss: 0.543339 Acc: 0.8750\n",
            "Train Epoch: 9 [33600/50000] Loss: 0.458184 Acc: 0.8750\n",
            "Train Epoch: 9 [35200/50000] Loss: 0.836268 Acc: 0.7500\n",
            "Train Epoch: 9 [36800/50000] Loss: 1.057175 Acc: 0.7500\n",
            "Train Epoch: 9 [38400/50000] Loss: 0.588591 Acc: 0.7500\n",
            "Train Epoch: 9 [40000/50000] Loss: 0.354387 Acc: 0.8750\n",
            "Train Epoch: 9 [41600/50000] Loss: 0.892189 Acc: 0.6875\n",
            "Train Epoch: 9 [43200/50000] Loss: 0.817091 Acc: 0.7500\n",
            "Train Epoch: 9 [44800/50000] Loss: 0.455410 Acc: 0.7500\n",
            "Train Epoch: 9 [46400/50000] Loss: 0.368269 Acc: 0.8750\n",
            "Train Epoch: 9 [48000/50000] Loss: 0.179861 Acc: 1.0000\n",
            "Train Epoch: 9 [49600/50000] Loss: 0.693278 Acc: 0.7500\n",
            "Elapsed 359.23s, 35.92 s/epoch, 0.01 s/batch, ets 0.00s\n",
            "\n",
            "Test set: Average loss: 0.8554, Accuracy: 7104/10000 (71%)\n",
            "\n",
            "Total time: 364.97, Best Loss: 0.784, Best Accuracy: 0.733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAy0TSpHJGbi",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">12. Plot Loss</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usF5DnpSJGbi",
        "lines_to_next_cell": 2,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "\n",
        "plt.figure\n",
        "plt.plot(x, epoch_train_loss, color='r', label=\"train loss\")\n",
        "plt.plot(x, epoch_test_loss, color='b', label=\"validation loss\")\n",
        "plt.xlabel('epoch no.')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my7EZy1MJGbl",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">13. Plot Accuracy</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_LPC69oJGbm",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "\n",
        "plt.figure\n",
        "plt.plot(x, epoch_train_acc, color='r', label=\"train accuracy\")\n",
        "plt.plot(x, epoch_test_acc, color='b', label=\"validation accuracy\")\n",
        "plt.xlabel('epoch no.')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='center right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_q1qekQJGbp",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">14. Loading the Model </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM-icBgDJGbp",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# initialize the model\n",
        "cnn_model = MyModel()\n",
        "\n",
        "models = 'models'\n",
        "\n",
        "model_file_name = 'cifar10_cnn_model.pt'\n",
        "\n",
        "model_path = os.path.join(models, model_file_name)\n",
        "\n",
        "# loading the model and getting model parameters by using load_state_dict\n",
        "cnn_model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF5uuusgJGbs",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">15. Model Prediction</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyAmSgK2JGbs",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "def prediction(model, train_config, batch_input):\n",
        "\n",
        "    # send model to cpu/cuda according to your system configuration\n",
        "    model.to(train_config.device)\n",
        "\n",
        "    # it is important to do model.eval() before prediction\n",
        "    model.eval()\n",
        "\n",
        "    data = batch_input.to(train_config.device)\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    # Score to probability using softmax\n",
        "    prob = F.softmax(output, dim=1)\n",
        "\n",
        "    # get the max probability\n",
        "    pred_prob = prob.data.max(dim=1)[0]\n",
        "\n",
        "    # get the index of the max probability\n",
        "    pred_index = prob.data.max(dim=1)[1]\n",
        "\n",
        "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaGRXs4HJGbx",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">16. Perform Inference on sample images </font>\n",
        "\n",
        "For prediction, we need to transform the data in the same way as we have done during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L5ecPKtJGbx",
        "lines_to_next_cell": 2,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "batch_size = 5\n",
        "train_config = TrainingConfiguration()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    train_config.device = \"cuda\"\n",
        "else:\n",
        "    train_config.device = \"cpu\"\n",
        "\n",
        "\n",
        "\n",
        "# load test data without image transformation\n",
        "test = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root=train_config.data_root, train=False, download=False,\n",
        "                   transform=transforms.functional.to_tensor),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=1\n",
        "    )\n",
        "\n",
        "try:\n",
        "    mean, std = get_mean_std_train_data(data_root)\n",
        "    assert len(mean) == len(std) == 3\n",
        "except:\n",
        "    mean = (0.5, 0.5, 0.5)\n",
        "    std = (0.5, 0.5, 0.5)\n",
        "\n",
        "# load testdata with image transformation\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "test_trans = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root=train_config.data_root, train=False, download=False, transform=image_transforms),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=1\n",
        "    )\n",
        "\n",
        "for data, _ in test_trans:\n",
        "    # pass the loaded model\n",
        "    pred, prob = prediction(cnn_model, train_config, data)\n",
        "    break\n",
        "\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (3, 3)\n",
        "for images, label in test:\n",
        "    for i, img in enumerate(images):\n",
        "        img = transforms.functional.to_pil_image(img)\n",
        "        plt.imshow(img)\n",
        "        plt.gca().set_title('Pred: {0}({1:0.2}), Label: {2}'.format(classes[pred[i]], prob[i], classes[label[i]]))\n",
        "        plt.show()\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvMswVmpJGb1"
      },
      "source": [
        "# <font style=\"color:blue\">17. Report your findings</font>\n",
        "-\n",
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVSVzjFJGb2",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "# <font style=\"color:blue\">References</font>\n",
        "\n",
        "1. https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "1. https://pytorch.org/tutorials/beginner/saving_loading_models.html"
      ]
    }
  ]
}